# Configuration for Topology-Aware nnU-Net Training

# Model architecture
in_channels: 1
out_channels: 1
initial_filters: 32  # Increased from 16 - larger model for better capacity
depth: 4  # Keep at 4 for reasonable training time

# Training parameters
batch_size: 2  # Adjust based on GPU memory
num_epochs: 100  # TRIAL RUN: reduced from 300 for validation
learning_rate: 0.0001  # CONSERVATIVE: stable starting point
weight_decay: 0.01  # Standard weight decay
warmup_epochs: 5  # Gradual warmup to prevent instability

# Data parameters
patch_size: [128, 128, 128]
num_workers: 4

# Cross-validation
n_folds: 5

# Loss weights - REBALANCED TO FIX FOREGROUND BIAS
# Previous: 99.88% FG vs 57.70% training labels (+42% overestimation)
# Fix: Increase focal + variance to penalize overconfident FG predictions
loss_weights:
  dice_weight: 0.2        # Reduce from 0.4 - dice encourages high overlap
  focal_weight: 0.4       # Increase from 0.2 - focus on hard negatives (background)
  variance_weight: 0.3    # Increase from 0.2 - force uncertainty, reduce overconfidence
  boundary_weight: 0.05   # ENABLE: surface quality matters
  cldice_weight: 0.05     # ENABLE: topology awareness
  connectivity_weight: 0.0  # Keep disabled for now
  
# Focal loss parameters (for handling class imbalance)
# Adjusted to focus more on background (hard negatives)
focal_gamma: 2.0  # Standard focal loss gamma
focal_alpha: 0.75  # Weight BACKGROUND heavily (inverse of FG foreground fraction ~0.66)

# Class weights for imbalance - CORRECTED
use_class_weights: true
background_weight: 2.94  # 1 / (1 - 0.66) for minority class
foreground_weight: 1.52  # 1 / 0.66 for majority class

# Learning rate scheduler
scheduler_type: "cosine_warm_restarts"  # Options: "plateau" or "cosine_warm_restarts"
scheduler_patience: 10  # Give time to learn before reducing LR (for plateau)
scheduler_factor: 0.5  # Standard reduction (for plateau)
T_0: 10  # Initial cycle length for cosine warm restarts
T_mult: 2  # Multiply cycle length after each restart
eta_min: 0.000001  # Minimum LR for cosine annealing
grad_clip_norm: 0.5  # AGGRESSIVE gradient clipping to prevent explosion

# Stochastic Weight Averaging (SWA)
swa_enabled: true
swa_start_epoch: 40  # Start SWA after this epoch (changed from 50 to 40)
swa_lr: 0.00005  # SWA learning rate (typically half of base LR)
swa_update_freq: 5  # Update averaged model every N epochs

# Weight noise injection (for plateau escape)
noise_enabled: true
noise_start_epoch: 30  # Start adding noise after this epoch
noise_frequency: 10  # Add noise every N epochs
noise_std: 0.001  # Noise standard deviation (0.1% of weights)
noise_decay: 0.9  # Decay noise over time
noise_target_layers: ['decoders', 'output']  # Only add noise to these layers

# Early stopping
early_stopping_patience: 50

# Post-processing (for validation)
postprocess:
  threshold: 0.5
  min_component_size: 100
  max_hole_size: 50
  morphology_kernel_size: 2
  separate_instances: false
