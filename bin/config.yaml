# Configuration for Topology-Aware nnU-Net Training

# Model architecture
in_channels: 1
out_channels: 1
initial_filters: 32  # Increased from 16 - larger model for better capacity
depth: 4  # Keep at 4 for reasonable training time

# Training parameters
batch_size: 8  # Increased from 2 - 100GB GPU available for faster learning
num_epochs: 300  # Extended training for convergence
learning_rate: 0.0002    # Increased from 0.0001 - scales with batch size
weight_decay: 0.01  # Standard weight decay
warmup_epochs: 5  # Gradual warmup to prevent instability

# Data parameters
patch_size: [128, 128, 128]
num_workers: 4

# Cross-validation
n_folds: 5

# Loss weights - FOCUSED ON 30-70% CLASS BALANCE WITH REGULARIZATION
# Goal: Train model to predict 30-70% background distribution
# Loss weights - FIXED TO ENABLE REAL LEARNING
# PROBLEM: variance_weight=0.3 was overwhelming Dice, causing model to ignore spatial structure
# SOLUTION: Prioritize Dice for spatial learning, use minimal variance regularization
loss_weights:
  dice_weight: 0.7        # PRIMARY: Boosted to escape variance plateau - heavily prioritize spatial learning
  focal_weight: 0.2       # Reduced slightly to make room for dice boost
  variance_weight: 0.05   # HEAVILY REDUCED: variance loss was stuck at 0.98, now minimal constraint
  boundary_weight: 0.05   # Edge detection
  cldice_weight: 0.0      # DISABLED: Too complex, disabled for stability
  connectivity_weight: 0.0  # Keep disabled

# Focal loss parameters
focal_gamma: 2.0  # Standard focal loss gamma
focal_alpha: 0.5  # Balanced attention to both classes

# Class weights for imbalance - CORRECTED
use_class_weights: true
background_weight: 2.94  # 1 / (1 - 0.66) for minority class
foreground_weight: 1.52  # 1 / 0.66 for majority class

# Learning rate scheduler
scheduler_type: "cosine_warm_restarts"  # Options: "plateau" or "cosine_warm_restarts"
scheduler_patience: 10  # Give time to learn before reducing LR (for plateau)
scheduler_factor: 0.5  # Standard reduction (for plateau)
T_0: 10  # Initial cycle length for cosine warm restarts
T_mult: 2  # Multiply cycle length after each restart
eta_min: 0.000001  # Minimum LR for cosine annealing
grad_clip_norm: 0.5  # AGGRESSIVE gradient clipping to prevent explosion

# Stochastic Weight Averaging (SWA)
swa_enabled: true
swa_start_epoch: 40  # Start SWA after this epoch (changed from 50 to 40)
swa_lr: 0.00005  # SWA learning rate (typically half of base LR)
swa_update_freq: 5  # Update averaged model every N epochs

# Weight noise injection (for plateau escape)
noise_enabled: true
noise_start_epoch: 30  # Start adding noise after this epoch
noise_frequency: 10  # Add noise every N epochs
noise_std: 0.001  # Noise standard deviation (0.1% of weights)
noise_decay: 0.9  # Decay noise over time
noise_target_layers: ['decoders', 'output']  # Only add noise to these layers

# Early stopping
early_stopping_patience: 50

# Catastrophic Degradation Detection & Recovery
# Automatically rolls back to best model if loss degrades by this fraction
# Example: threshold=0.15 means rollback if loss > 1.15x best loss
catastrophic_degradation_threshold: 0.15  # 15% increase triggers rollback
# Options:
#   0.10 (10%)  - Very aggressive, catches early degradation
#   0.15 (15%)  - Balanced (recommended - default)
#   0.25 (25%)  - Conservative, only catches major degradation
# Options:
#   0.10 (10%)  - Very aggressive, catches early degradation
#   0.15 (15%)  - Balanced (recommended - default)
#   0.25 (25%)  - Conservative, only catches major degradation

# Post-processing (for validation)
postprocess:
  threshold: 0.5
  min_component_size: 100
  max_hole_size: 50
  morphology_kernel_size: 2
  separate_instances: false
