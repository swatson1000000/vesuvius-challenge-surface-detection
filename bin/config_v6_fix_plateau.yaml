# Configuration for Topology-Aware nnU-Net Training - v6_fix_plateau
# Target: BREAK the post-epoch-5 plateau trap
# Strategy: Simplify loss, reduce variance penalty, stabilize LR scaling

# Model architecture
in_channels: 1
out_channels: 1
initial_filters: 32
depth: 4

# Training parameters
batch_size: 4              # Increase batch size (was 2) - more stable gradients
num_epochs: 100            # Reset to reasonable number
learning_rate: 0.001      # Standard starting LR (not 0.0001)
weight_decay: 0.0001      # Conservative regularization
warmup_epochs: 2          # Minimal warmup

# Data parameters
patch_size: [128, 128, 128]
num_workers: 4

# Cross-validation
n_folds: 5

# Loss weights - SIMPLIFIED (NO variance trap)
loss_weights:
  dice_weight: 0.8        # STRONG: Primary objective
  focal_weight: 0.15      # Moderate: Handle hard cases
  variance_weight: 0.0    # DISABLED: Variance loss is the plateau culprit
  boundary_weight: 0.05   # Keep topology awareness
  cldice_weight: 0.0      # Disable
  connectivity_weight: 0.0

# Focal loss parameters
focal_gamma: 2.0
focal_alpha: 0.35

# Class weights - BALANCED
use_class_weights: true
background_weight: 1.0
foreground_weight: 1.5    # Gentle - don't over-penalize

# Learning rate scheduler
scheduler_type: "plateau"
scheduler_patience: 5
scheduler_factor: 0.5
scheduler_threshold: 0.001

# SWA - disabled for fresh start
swa_enabled: false

# Noise - disabled (was causing chaos with high LR)
noise_enabled: false

# Early stopping - reasonable
early_stopping_patience: 20
